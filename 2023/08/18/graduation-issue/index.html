<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="WuXin">





<title>基于和Word2vec和BiLSTM的微博用户情感分析——以新十条”疫情政策为例 | Hexo</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    
    <link rel="stylesheet" href="/../fonts/iconfont2/iconfont.css">
    
    <link rel="stylesheet" href="/../fonts/iconfont3/iconfont.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Elva&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">📑Posts</a>
                
                    <a class="menu-item" href="/category">🗃Categories</a>
                
                    <a class="menu-item" href="/tag">🔖Tags</a>
                
                    <a class="menu-item" href="/about">💡About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Elva&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">📑Posts</a>
                
                    <a class="menu-item" href="/category">🗃Categories</a>
                
                    <a class="menu-item" href="/tag">🔖Tags</a>
                
                    <a class="menu-item" href="/about">💡About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">基于和Word2vec和BiLSTM的微博用户情感分析——以新十条”疫情政策为例</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">WuXin</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">August 18, 2023&nbsp;&nbsp;22:03:39</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/">项目实战</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h2 id="基于和Word2vec和BiLSTM的微博用户情感分析——以新十条”疫情政策为例"><a href="#基于和Word2vec和BiLSTM的微博用户情感分析——以新十条”疫情政策为例" class="headerlink" title="基于和Word2vec和BiLSTM的微博用户情感分析——以新十条”疫情政策为例"></a>基于和Word2vec和BiLSTM的微博用户情感分析——以新十条”疫情政策为例</h2><blockquote>
<p>写在前面：通过python爬虫获取数据并进行数据预处理，将得到的数据可视化，同时将数据通过word2vec模型完成文本特征提取。一方面，将提取的句子特征输入CNN-BiLSTM模型，用训练完毕的模型预测所有评论的情感倾向，以进行后续的结果可视化、总体分析、词频分析和情感变化分析。另一方面用得到的word2vec模型进行相似词分析，获知主体对象在该次热点事件中面临的公众态度。</p>
</blockquote>
<h3 id="1-研究结构"><a href="#1-研究结构" class="headerlink" title="1. 研究结构"></a>1. 研究结构</h3><p><img src="/pic/graduation_issue/image-20230803175045288.png" alt="image-20230803175045288"></p>
<h3 id="2-数据的获取和预处理"><a href="#2-数据的获取和预处理" class="headerlink" title="2. 数据的获取和预处理"></a>2. 数据的获取和预处理</h3><h4 id="2-1-数据的获取"><a href="#2-1-数据的获取" class="headerlink" title="2.1 数据的获取"></a>2.1 数据的获取</h4><p>本文从“新十条”发布后一些热门词条下评论数较多且未设置精选评论的微博，具体词条如表所示：</p>
<table>
<thead>
<tr>
<th>发布日期</th>
<th>词条</th>
</tr>
</thead>
<tbody><tr>
<td>2022-12-5</td>
<td>#抗疫3年其实我们清零了很多次#</td>
</tr>
<tr>
<td>2022-12-7</td>
<td>#新十条#</td>
</tr>
<tr>
<td>2022-12-10</td>
<td>#年轻人阳性了可以不吃药吗#</td>
</tr>
<tr>
<td>2022-12-12</td>
<td>#新冠感染者用药目录#</td>
</tr>
<tr>
<td>2022-12-13</td>
<td>#行程卡下线#</td>
</tr>
<tr>
<td>2022-12-14</td>
<td>#专家称新冠肺炎最终会降为丙类丙管#</td>
</tr>
<tr>
<td>2022-12-15</td>
<td>#布洛芬和连花清瘟能一起吃吗#</td>
</tr>
<tr>
<td>2022-12-15</td>
<td>#阳性康复后还有传染性吗#</td>
</tr>
<tr>
<td>2022-12-18</td>
<td>#无症状感染者为什么越来越少了#</td>
</tr>
<tr>
<td>2022-12-19</td>
<td>#感染新冠发病7天内可能出现这些症状#</td>
</tr>
<tr>
<td>2022-12-21</td>
<td>#每个人可能都会感染奥密克戎#</td>
</tr>
<tr>
<td>2022-12-22</td>
<td>#阳后7天是否就没传染性了#</td>
</tr>
<tr>
<td>2022-12-24</td>
<td>#家有老人面对新冠要知道的事#</td>
</tr>
<tr>
<td>2022-12-26</td>
<td>#老人阳了一定要查血氧#</td>
</tr>
<tr>
<td>2022-12-26</td>
<td>#阳了一直咳会不会得肺炎#</td>
</tr>
<tr>
<td>2022-12-27</td>
<td>#调整为乙类乙管不是放任不管#</td>
</tr>
<tr>
<td>2022-12-30</td>
<td><a target="_blank" rel="noopener" href="https://m.weibo.cn/search?containerid=231522type=1&amp;t=10&amp;q=%23%E8%82%BA%E9%83%A8CT%E6%A3%80%E6%9F%A5%E9%9C%80%E6%B1%82%E6%9A%B4%E5%A2%9E%23&amp;extparam=%23%E8%82%BA%E9%83%A8CT%E6%A3%80%E6%9F%A5%E9%9C%80%E6%B1%82%E6%9A%B4%E5%A2%9E%23&amp;luicode=10000011&amp;lfid=231522type=1&amp;t=10&amp;q=%23%E4%BD%93%E6%A3%80%E6%9C%BA%E6%9E%84%E9%98%B3%E5%BA%B7%E4%BD%93%E6%A3%80%E5%A5%97%E9%A4%90%E9%94%80%E9%87%8F%E7%A0%B414%E4%B8%87%23">#肺部CT检查需求暴增#</a></td>
</tr>
<tr>
<td>2022-12-30</td>
<td>#新冠病毒感染者恢复期健康指引#</td>
</tr>
</tbody></table>
<p>本文利用的工具是Python，构建接口从<a href="weibo.com">微博网页版</a>和<a href="m.weibo.cn">触屏版</a>抓取<strong>评论以及评论用户id</strong>，代码参考了以下两篇文章：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://buyixiao.github.io/blog/weibo-super-spider.html#%E8%AF%84%E8%AE%BA%E7%88%AC%E8%99%AB">【稳定可用 | 持续更新】微博超级爬虫 | BuyiXiao’s Blog</a></li>
<li><a target="_blank" rel="noopener" href="https://www.52pojie.cn/thread-1623485-1-1.html">爬取微博评论突破50页限制 - 『编程语言区』 - 吾爱破解 - LCG - LSG |安卓破解|病毒分析|www.52pojie.cn</a></li>
</ul>
<p>需要说明的是，抓取的评论存在大量重复的情况，在抓取评论后需要进行<strong>去重</strong>，此后，还需要根据用户id从用户主页抓取<strong>公开的个人信息</strong>，参考代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">headers = {</span><br><span class="line">    <span class="string">'authority'</span>: <span class="string">'weibo.com'</span>,</span><br><span class="line">    <span class="string">'x-requested-with'</span>: <span class="string">'XMLHttpRequest'</span>,</span><br><span class="line">    <span class="string">'sec-ch-ua-mobile'</span>: <span class="string">'?0'</span>,</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'content-type'</span>: <span class="string">'application/x-www-form-urlencoded'</span>,</span><br><span class="line">    <span class="string">'accept'</span>: <span class="string">'*/*'</span>,</span><br><span class="line">    <span class="string">'sec-fetch-site'</span>: <span class="string">'same-origin'</span>,</span><br><span class="line">    <span class="string">'sec-fetch-mode'</span>: <span class="string">'cors'</span>,</span><br><span class="line">    <span class="string">'sec-fetch-dest'</span>: <span class="string">'empty'</span>,</span><br><span class="line">    <span class="string">'referer'</span>: <span class="string">'https://weibo.com/1887344341/MibHq7TvH?filter=hot&amp;root_comment_id=0&amp;type=comment'</span>,</span><br><span class="line">    <span class="string">'accept-language'</span>: <span class="string">'zh-CN,zh;q=0.9,en-CN;q=0.8,en;q=0.7,es-MX;q=0.6,es;q=0.5'</span>,</span><br><span class="line">    <span class="string">'cookie'</span>: <span class="string">''</span> <span class="comment">#input your cookie</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getUserInfo</span>(<span class="params">uid</span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    根据uid抓取用户信息</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        uid = <span class="built_in">int</span>(uid)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        uid = parseUid(uid)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> uid:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    response = requests.get(url=<span class="string">f'https://weibo.com/ajax/profile/detail?uid=<span class="subst">{uid}</span>'</span>, headers=headers)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">400</span>:</span><br><span class="line">        <span class="keyword">return</span> {</span><br><span class="line">            <span class="string">'sunshine_credit_level'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'school'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'location'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'gender'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'birthday'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'created_at'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'description'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'followers_num'</span>: <span class="literal">None</span></span><br><span class="line">        }</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        resp_json = response.json().get(<span class="string">'data'</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"出错了"</span>, e)</span><br><span class="line">        <span class="keyword">return</span> {</span><br><span class="line">            <span class="string">'sunshine_credit_level'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'school'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'location'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'gender'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'birthday'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'created_at'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'description'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'followers_num'</span>: <span class="literal">None</span></span><br><span class="line">        }</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> resp_json:</span><br><span class="line">        <span class="keyword">return</span> {</span><br><span class="line">            <span class="string">'sunshine_credit_level'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'school'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'location'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'gender'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'birthday'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'created_at'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'description'</span>: <span class="literal">None</span>,</span><br><span class="line">            <span class="string">'followers_num'</span>: <span class="literal">None</span></span><br><span class="line">        }</span><br><span class="line">    sunshine_credit = resp_json.get(<span class="string">'sunshine_credit'</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> sunshine_credit:</span><br><span class="line">        sunshine_credit_level = sunshine_credit.get(<span class="string">'level'</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        sunshine_credit_level = <span class="literal">None</span></span><br><span class="line">    education = resp_json.get(<span class="string">'education'</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> education:</span><br><span class="line">        school = education.get(<span class="string">'school'</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        school = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    location = resp_json.get(<span class="string">'ip_location'</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> location <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        location = location.strip(<span class="string">'IP属地：'</span>)</span><br><span class="line">    gender = resp_json.get(<span class="string">'gender'</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    birthday = resp_json.get(<span class="string">'birthday'</span>, <span class="literal">None</span>)</span><br><span class="line">    created_at = resp_json.get(<span class="string">'created_at'</span>, <span class="literal">None</span>)</span><br><span class="line">    description = resp_json.get(<span class="string">'description'</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="comment"># 我关注的人中有多少人关注 ta</span></span><br><span class="line">    followers = resp_json.get(<span class="string">'followers'</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> followers:</span><br><span class="line">        followers_num = followers.get(<span class="string">'total_number'</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        followers_num = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> {</span><br><span class="line">        <span class="string">'sunshine_credit_level'</span>: sunshine_credit_level,</span><br><span class="line">        <span class="string">'school'</span>: school,</span><br><span class="line">        <span class="string">'location'</span>: location,</span><br><span class="line">        <span class="string">'gender'</span>: gender,</span><br><span class="line">        <span class="string">'birthday'</span>: birthday,</span><br><span class="line">        <span class="string">'created_at'</span>: created_at,</span><br><span class="line">        <span class="string">'description'</span>: description,</span><br><span class="line">        <span class="string">'followers_num'</span>: followers_num</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseUid</span>(<span class="params">uid</span>):</span><br><span class="line">    response = requests.get(url=<span class="string">f'https://weibo.com/ajax/profile/info?custom=<span class="subst">{uid}</span>'</span>, headers=headers)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> response.json()[<span class="string">'data'</span>][<span class="string">'user'</span>][<span class="string">'id'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfAddUserInfo</span>(<span class="params">file_path, user_col, user_info_col=<span class="string">'user_info'</span></span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @params file_path 指定路径</span></span><br><span class="line"><span class="string">    @params user_col 指定用户主页链接在那一列, 比如评论csv文件的是 comment_user_link</span></span><br><span class="line"><span class="string">    @params user_info_col 指定新加的 userinfo 列名，默认是 user_info</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    df = pd.read_excel(file_path)</span><br><span class="line">    user_info_init_value = <span class="string">'init'</span></span><br><span class="line">    columns = df.columns.values.tolist()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> user_info_col <span class="keyword">in</span> columns:</span><br><span class="line">        df[user_info_col] = [user_info_init_value <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(df.shape[<span class="number">0</span>])]</span><br><span class="line">    <span class="keyword">for</span> index, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f'   <span class="subst">{index+<span class="number">1</span>}</span>/<span class="subst">{df.shape[<span class="number">0</span>]}</span>   '</span>)</span><br><span class="line">        <span class="keyword">if</span> (index+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            df.to_csv(file_path, index=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> row.get(user_info_col, user_info_init_value) == user_info_init_value:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">'skip'</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        user_link = row[user_col]</span><br><span class="line">        user_id = user_link[user_link.rindex(<span class="string">'/'</span>)+<span class="number">1</span>:]</span><br><span class="line">        user_info = getUserInfo(user_id)</span><br><span class="line">        <span class="built_in">print</span>(user_info)</span><br><span class="line">        <span class="keyword">if</span> user_info:</span><br><span class="line">            <span class="comment"># 在 user_info 中统一为 user_link</span></span><br><span class="line">            user_info[<span class="string">'user_link'</span>] = user_link</span><br><span class="line">            df.loc[index, user_info_col] = json.dumps(user_info, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">            sleep(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(user_link)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        df.to_excel(file_path, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">从已经加好 userinfo 的 df 里遍历 userinfo </span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfGetUserInfo</span>(<span class="params">file_path, user_info_col</span>):</span><br><span class="line">    df = pd.read_excel(file_path)</span><br><span class="line">    <span class="keyword">for</span> index, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">        user_info = json.loads(row[user_info_col])</span><br><span class="line">        location = user_info[<span class="string">'location'</span>]</span><br><span class="line">        user_link = user_info[<span class="string">'user_link'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(location, user_link)</span><br></pre></td></tr></tbody></table></figure>



<h4 id="2-2-数据预处理之清洗"><a href="#2-2-数据预处理之清洗" class="headerlink" title="2.2 数据预处理之清洗"></a>2.2 数据预处理之清洗</h4><p>在获取数据后，需调用Python中的re模块，利用正则表达式去除数据中的无效信息，包括：html格式、#话题名称、@人名、链接以及“转发微博”、“回复”等文本，以获得有效的评论内容。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面的处理可以用循环简略</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean</span>(<span class="params">comment</span>):</span><br><span class="line">    comment = <span class="built_in">str</span>(comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"转发微博"</span>,<span class="string">""</span>, comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">'回复'</span>,<span class="string">''</span>,new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">'&lt;.*?&gt;'</span>,<span class="string">''</span>,new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"&lt;.*?:"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"&lt;.*?："</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"//.*?:"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">'@.*?&lt;/a&gt;'</span>,<span class="string">''</span>,new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"@.*? "</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"@.*?:"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"@.*?："</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"[.*?]"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"@[\\u4e00-\\u9fa5\\w\\-]+"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    </span><br><span class="line">    new_comment = re.sub(<span class="string">"查看图片"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"【.*?】"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"[.*?]"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"#.*?#"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">'回复.*?&gt;：'</span>,<span class="string">''</span>,new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">'回复.*?&gt;:'</span>,<span class="string">''</span>,new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"网页链接"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"查看图片"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">r"\[\S+\]"</span>, <span class="string">""</span>, new_comment) </span><br><span class="line">    new_comment = re.sub(<span class="string">':'</span>,<span class="string">''</span>,new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">'：'</span>,<span class="string">''</span>,new_comment)</span><br><span class="line">    <span class="keyword">return</span> new_comment</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">filter_emoji</span>(<span class="params">desstr, restr=<span class="string">''</span></span>):</span><br><span class="line">    <span class="comment"># 过滤表情</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        co = re.<span class="built_in">compile</span>(<span class="string">u'[\U00010000-\U0010ffff]'</span>)</span><br><span class="line">    <span class="keyword">except</span> re.error:</span><br><span class="line">        co = re.<span class="built_in">compile</span>(<span class="string">u'[\uD800-\uDBFF][\uDC00-\uDFFF]'</span>)</span><br><span class="line">    <span class="keyword">return</span> co.sub(restr, desstr)</span><br></pre></td></tr></tbody></table></figure>

<p>以下面这条评论为例：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clean(<span class="string">'回复&lt;a href=/n/秋风里的旧少年&gt;@秋风里的旧少年&lt;/a&gt;:我的意思是我没在武汉呆过罢了&lt;img alt="[兔子]" title="[兔子]" src="https://face.t.sinajs.cn/t4/appstyle/expression/ext/normal/c6/2018new_tuzi_org.png" /&gt;我知道你很急但你先别急'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>结果是：</p>
<figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我的意思是我没在武汉呆过罢了我知道你很急但你先别急</span><br></pre></td></tr></tbody></table></figure>

<p>经过上述处理后，剩余的有效数据共26920条。</p>
<p><img src="/pic/graduation_issue/image-20230807213411813.png" alt="image-20230807213411813"></p>
<h4 id="2-3-数据预处理之分词"><a href="#2-3-数据预处理之分词" class="headerlink" title="2.3 数据预处理之分词"></a>2.3 数据预处理之分词</h4><p>在利用word2vec获取词向量之前，还需对所有评论进行分词，Python中第三方库jieba是针对中文文本分词的常用工具，为了使得分词结果更加准确，我们将情绪词典和部分主题词汇作为添加到jieba库中的自定义词典中。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">jieba.load_userdict(<span class="string">'jieba_sentiment.txt'</span>) <span class="comment"># 导入情感词典，避免情感词被分词</span></span><br><span class="line">jieba.del_word(<span class="string">'药都'</span>) <span class="comment">#根据分词结果适当调整</span></span><br></pre></td></tr></tbody></table></figure>

<p>为了之后适用于Word2vec模型，需要将每一条评论分词并将结果存入txt文件中</p>
<figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将列表转化为.txt文件</span></span><br><span class="line">file_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> comment <span class="keyword">in</span> comments:</span><br><span class="line">    file_path = <span class="string">'filepath\{0}.txt'</span>.<span class="built_in">format</span>(file_count) <span class="comment">#改为自己的文件路径</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(comment) != <span class="built_in">str</span>:</span><br><span class="line">            comment = <span class="built_in">str</span>(comment)</span><br><span class="line">        f.write(comment)</span><br><span class="line">    file_count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'第{0}个文件写入完成'</span>.<span class="built_in">format</span>(file_count))</span><br><span class="line"><span class="comment"># 同时将文件分词后存入一个文本文件中</span></span><br><span class="line">    <span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">26925</span>):</span><br><span class="line">        inputfilename = <span class="string">'input_path/{}.txt'</span>.<span class="built_in">format</span>(j) <span class="comment">#改为自己的文件路径</span></span><br><span class="line">        outputfilename =  <span class="string">'output_path/weibosentiment.txt'</span> <span class="comment">#改为自己的文件路径</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span> (inputfilename,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f1:</span><br><span class="line">            comment = f1.read().strip()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span> (outputfilename,<span class="string">'a+'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f2:</span><br><span class="line">            words = jieba.lcut(comment)</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">                f2.write(word + <span class="string">' '</span>)</span><br><span class="line">            f2.write(<span class="string">'\n'</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'第{0}个文件写入完成'</span>.<span class="built_in">format</span>(j+<span class="number">1</span>))</span><br></pre></td></tr></tbody></table></figure>

<p>结果如下：</p>
<p><img src="/pic/graduation_issue/image-20230807214341661.png" alt="image-20230807214341661"><img src="/pic/graduation_issue/image-20230807214341738.png" alt="image-20230807214341738"></p>
<p>分词结果汇总txt文件：</p>
<p><img src="/pic/graduation_issue/image-20230807214319759.png" alt="image-20230807214319759"></p>
<h3 id="3-数据描述性统计"><a href="#3-数据描述性统计" class="headerlink" title="3. 数据描述性统计"></a>3. 数据描述性统计</h3><h4 id="3-1-文本长度统计"><a href="#3-1-文本长度统计" class="headerlink" title="3.1 文本长度统计"></a>3.1 文本长度统计</h4><p>读入上述汇总的分词txt文件，统计数据集中句子长度与相应数量，</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">len_count</span>(<span class="params">data</span>):</span><br><span class="line">    res = []</span><br><span class="line">    i=<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> comment <span class="keyword">in</span> data:</span><br><span class="line">        l = <span class="built_in">len</span>(comment)</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line">        res.append(l)</span><br><span class="line">    <span class="built_in">dict</span> = {} </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> res:         </span><br><span class="line">        <span class="built_in">dict</span>[key] = <span class="built_in">dict</span>.get(key, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    result = <span class="built_in">sorted</span>(<span class="built_in">dict</span>.items(),key = <span class="keyword">lambda</span> x:x[<span class="number">0</span>],reverse = <span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>, result</span><br><span class="line"></span><br><span class="line">maxlen = <span class="built_in">max</span>(len_dict.keys(),key=(<span class="keyword">lambda</span> x:x))</span><br><span class="line"><span class="built_in">print</span>(maxlen)</span><br><span class="line"></span><br><span class="line">len_list = []</span><br><span class="line">len_count_list = []</span><br><span class="line"><span class="keyword">for</span> single <span class="keyword">in</span> result:</span><br><span class="line">    len_list.append(single[<span class="number">0</span>])</span><br><span class="line">    len_count_list.append(single[<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    x_data = len_list</span><br><span class="line">y_data =len_count_list</span><br><span class="line">matplotlib.rc(<span class="string">'axes'</span>, facecolor = <span class="string">'white'</span>)</span><br><span class="line">matplotlib.rc(<span class="string">'figure'</span>, figsize = (<span class="number">80</span>,<span class="number">25</span>))</span><br><span class="line">matplotlib.rc(<span class="string">'axes'</span>, grid = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.bar(x = x_data, height=y_data,color=<span class="string">'dodgerblue'</span>,edgecolor=<span class="string">'black'</span>,linewidth=<span class="number">8</span>)</span><br><span class="line">plt.xticks(fontsize=<span class="number">50</span>)</span><br><span class="line">plt.yticks(fontsize=<span class="number">50</span>)</span><br><span class="line">plt.xlim(-<span class="number">2</span>,<span class="number">100</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>,<span class="number">2000</span>)</span><br><span class="line">plt.savefig(<span class="string">'D:/BaiduNetdiskDownload/comment/comment/comment_txt/result/wordcount.png'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="/pic/graduation_issue/image-20230807214658682.png" alt="image-20230807214658682"></p>
<p>结果显示数据集中评论的长度集中在0~20之间，长度在90以上的评论较少，因此本文的情感分析方法应当选择针对短文本的算法。</p>
<h4 id="3-2-评论地区可视化"><a href="#3-2-评论地区可视化" class="headerlink" title="3.2 评论地区可视化"></a>3.2 评论地区可视化</h4><p>统计不同地区的评论数：                 </p>
<p><img src="/pic/graduation_issue/image-20230807214803122.png" alt="image-20230807214803122"></p>
<p>上述与《微博2020用户发展报告》中指出微博覆盖京津冀、长三角、闵三角、珠三角和川渝地区最多相符</p>
<p><img src="/pic/graduation_issue/image-20230807214808403.png" alt="image-20230807214808403"></p>
<h3 id="4-训练集准备——利用百度API批量进行情感分析"><a href="#4-训练集准备——利用百度API批量进行情感分析" class="headerlink" title="4. 训练集准备——利用百度API批量进行情感分析"></a>4. 训练集准备——利用百度API批量进行情感分析</h3><p>为了情感标注更加客观，采用百度API平台批量进行情感分析并且人工核对的方式对抽取的部分数据进行标注。</p>
<p>参考了以下文章：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43542339/article/details/105181078">百度api——python大批量数据情感分析_开着奔驰种地的博客-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/142011031">基于词典的文本情感分析（附代码） - 知乎 (zhihu.com)</a></p>
<h4 id="4-1-ACCESS-TOKEN的获取"><a href="#4-1-ACCESS-TOKEN的获取" class="headerlink" title="4.1 ACCESS TOKEN的获取"></a>4.1 ACCESS TOKEN的获取</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先获取百度AI平台的Access Token</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> ssl</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_access_token</span>():</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    获取百度AI平台的Access Token</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    host = <span class="string">'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&amp;client_id=[API Key]&amp;client_secret=[Secret Key]'</span></span><br><span class="line">    request = urllib.request.Request(host)</span><br><span class="line">    request.add_header(<span class="string">'Content-Type'</span>, <span class="string">'application/json; charset=UTF-8'</span>)</span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    rdata = json.loads(content)</span><br><span class="line">    <span class="keyword">return</span> rdata[<span class="string">'access_token'</span>]</span><br><span class="line"><span class="built_in">print</span>(get_access_token())</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>



<h4 id="4-2-连接百度云并批量处理"><a href="#4-2-连接百度云并批量处理" class="headerlink" title="4.2 连接百度云并批量处理"></a>4.2 连接百度云并批量处理</h4><p>将以下文件放入同一个文件夹中：</p>
<ol>
<li>utils.py文件</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ToolGeneral</span>():</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Tool function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">is_odd</span>(<span class="params">self,num</span>):</span><br><span class="line">        <span class="keyword">if</span> num % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">'even'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">'odd'</span>        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_dict</span>(<span class="params">self,file</span>):  </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Load dictionary</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">with</span>  <span class="built_in">open</span>(file,encoding=<span class="string">'utf-8'</span>,errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            lines = fp.readlines()</span><br><span class="line">            lines = [l.strip() <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"Load data from file (%s) finished !"</span>%file)</span><br><span class="line">            dictionary = [word.strip() <span class="keyword">for</span> word <span class="keyword">in</span> lines]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">set</span>(dictionary)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sentence_split_regex</span>(<span class="params">self,sentence</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Segmentation of sentence</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> sentence <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            sentence = re.sub(<span class="string">r"&amp;ndash;+|&amp;mdash;+"</span>, <span class="string">"-"</span>, sentence)</span><br><span class="line">            sub_sentence = re.split(<span class="string">r"([。,，！!？?;；\s…~～]+|\.{2,}|&amp;hellip;+|&amp;nbsp+|_n|_t)"</span>, sentence)</span><br><span class="line">            sub_sentence.append(<span class="string">""</span>)</span><br><span class="line">            sub_sentence =  [<span class="string">""</span>.join(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">zip</span>(sub_sentence[<span class="number">0</span>::<span class="number">2</span>], sub_sentence[<span class="number">1</span>::<span class="number">2</span>])]</span><br><span class="line">            sub_sentence = [s <span class="keyword">for</span> s <span class="keyword">in</span> sub_sentence <span class="keyword">if</span> s != <span class="string">''</span>]</span><br><span class="line">            <span class="keyword">if</span> sub_sentence != []:</span><br><span class="line">                <span class="keyword">return</span> sub_sentence</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> [sentence]</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>: </span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    tool = ToolGeneral()    </span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    s = <span class="string">'我今天。昨天上午，还有现在'</span></span><br><span class="line">    ls = tool.sentence_split_regex(s)</span><br><span class="line">    <span class="built_in">print</span>(ls)              </span><br></pre></td></tr></tbody></table></figure>

<ol start="2">
<li>hyperparameters.py文件</li>
</ol>
<figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Mon Jan  6 20:44:08 2020</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: cm</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> ToolGeneral</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pwd = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">tool = ToolGeneral()    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Hyperparams</span>:</span><br><span class="line">    <span class="string">'''Hyper parameters'''</span></span><br><span class="line">    <span class="comment"># Load sentiment dictionary</span></span><br><span class="line">    deny_word = tool.load_dict(os.path.join(pwd,<span class="string">'dict'</span>,<span class="string">'not.txt'</span>))</span><br><span class="line">    posdict = tool.load_dict(os.path.join(pwd,<span class="string">'dict'</span>,<span class="string">'positive.txt'</span>))</span><br><span class="line">    negdict = tool.load_dict(os.path.join(pwd,<span class="string">'dict'</span>, <span class="string">'negative.txt'</span>))</span><br><span class="line">    pos_neg_dict = posdict|negdict</span><br><span class="line">    <span class="comment"># Load adverb dictionary</span></span><br><span class="line">    mostdict = tool.load_dict(os.path.join(pwd,<span class="string">'dict'</span>,<span class="string">'most.txt'</span>))</span><br><span class="line">    verydict = tool.load_dict(os.path.join(pwd,<span class="string">'dict'</span>,<span class="string">'very.txt'</span>))</span><br><span class="line">    moredict = tool.load_dict(os.path.join(pwd,<span class="string">'dict'</span>,<span class="string">'more.txt'</span>))</span><br><span class="line">    ishdict = tool.load_dict(os.path.join(pwd,<span class="string">'dict'</span>,<span class="string">'ish.txt'</span>))</span><br><span class="line">    insufficientlydict = tool.load_dict(os.path.join(pwd,<span class="string">'dict'</span>,<span class="string">'insufficiently.txt'</span>))</span><br><span class="line">    overdict = tool.load_dict(os.path.join(pwd,<span class="string">'dict'</span>,<span class="string">'over.txt'</span>))</span><br><span class="line">    inversedict = tool.load_dict(os.path.join(pwd,<span class="string">'dict'</span>,<span class="string">'inverse.txt'</span>))</span><br></pre></td></tr></tbody></table></figure>

<ol start="3">
<li>networks.py文件</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Wed Oct 25 11:48:24 2017</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: cm</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">sys.path.append(os.path.dirname(os.path.dirname(__file__)))</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> ToolGeneral</span><br><span class="line"><span class="keyword">from</span> hyperparameters <span class="keyword">import</span> Hyperparams <span class="keyword">as</span> hp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tool = ToolGeneral()</span><br><span class="line">jieba.load_userdict(os.path.join(os.path.dirname(os.path.abspath(__file__)), <span class="string">'dict'</span>,<span class="string">'jieba_sentiment.txt'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SentimentAnalysis</span>():</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Sentiment Analysis with some dictionarys</span></span><br><span class="line"><span class="string">    """</span>      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sentiment_score_list</span>(<span class="params">self,dataset</span>):</span><br><span class="line">        seg_sentence = tool.sentence_split_regex(dataset)</span><br><span class="line">        count1,count2 = [],[]</span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> seg_sentence: </span><br><span class="line">            words = jieba.lcut(sentence, cut_all=<span class="literal">False</span>)</span><br><span class="line">            <span class="built_in">print</span>(words)</span><br><span class="line">            i = <span class="number">0</span> </span><br><span class="line">            a = <span class="number">0</span> </span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">                <span class="string">"""</span></span><br><span class="line"><span class="string">                poscount 积极词的第一次分值;</span></span><br><span class="line"><span class="string">                poscount2 积极反转后的分值;</span></span><br><span class="line"><span class="string">                poscount3 积极词的最后分值（包括叹号的分值）      </span></span><br><span class="line"><span class="string">                """</span></span><br><span class="line">                poscount,negcount,poscount2,negcount2,poscount3,negcount3 = <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>  <span class="comment"># </span></span><br><span class="line">                <span class="keyword">if</span> word <span class="keyword">in</span> hp.posdict : </span><br><span class="line">                    <span class="keyword">if</span> word <span class="keyword">in</span> [<span class="string">'好'</span>,<span class="string">'真'</span>,<span class="string">'实在'</span>] <span class="keyword">and</span> words[<span class="built_in">min</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(words)-<span class="number">1</span>)] <span class="keyword">in</span> hp.pos_neg_dict  <span class="keyword">and</span> words[<span class="built_in">min</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(words)-<span class="number">1</span>)] != word:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        poscount +=<span class="number">1</span></span><br><span class="line">                        c = <span class="number">0</span></span><br><span class="line">                        <span class="keyword">for</span> w <span class="keyword">in</span> words[a:i]: <span class="comment"># 扫描情感词前的程度词</span></span><br><span class="line">                            <span class="keyword">if</span> w <span class="keyword">in</span> hp.mostdict:</span><br><span class="line">                                poscount *= <span class="number">4</span></span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.verydict:</span><br><span class="line">                                poscount *= <span class="number">3</span> </span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.moredict:</span><br><span class="line">                                poscount *= <span class="number">2</span> </span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.ishdict:</span><br><span class="line">                                poscount *= <span class="number">0.5</span></span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.insufficientlydict:</span><br><span class="line">                                poscount *= -<span class="number">0.3</span> </span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.overdict:</span><br><span class="line">                                poscount *= -<span class="number">0.5</span> </span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.inversedict: </span><br><span class="line">                                c+= <span class="number">1</span></span><br><span class="line">                            <span class="keyword">else</span>:</span><br><span class="line">                                poscount *= <span class="number">1</span></span><br><span class="line">                        <span class="keyword">if</span> tool.is_odd(c) == <span class="string">'odd'</span>: <span class="comment"># 扫描情感词前的否定词数</span></span><br><span class="line">                            poscount *= -<span class="number">1.0</span></span><br><span class="line">                            poscount2 += poscount</span><br><span class="line">                            poscount = <span class="number">0</span></span><br><span class="line">                            poscount3 = poscount + poscount2 + poscount3</span><br><span class="line">                            poscount2 = <span class="number">0</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            poscount3 = poscount + poscount2 + poscount3</span><br><span class="line">                            poscount = <span class="number">0</span></span><br><span class="line">                        a = i+<span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> word <span class="keyword">in</span> hp.negdict: <span class="comment"># 消极情感的分析，与上面一致              </span></span><br><span class="line">                    <span class="keyword">if</span> word <span class="keyword">in</span> [<span class="string">'好'</span>,<span class="string">'真'</span>,<span class="string">'实在'</span>] <span class="keyword">and</span> words[<span class="built_in">min</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(words)-<span class="number">1</span>)] <span class="keyword">in</span> hp.pos_neg_dict <span class="keyword">and</span> words[<span class="built_in">min</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(words)-<span class="number">1</span>)] != word:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        negcount += <span class="number">1</span></span><br><span class="line">                        d = <span class="number">0</span></span><br><span class="line">                        <span class="keyword">for</span> w <span class="keyword">in</span> words[a:i]:</span><br><span class="line">                            <span class="keyword">if</span> w <span class="keyword">in</span> hp.mostdict:</span><br><span class="line">                                negcount *= <span class="number">4</span></span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.verydict:</span><br><span class="line">                                negcount *= <span class="number">3</span></span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.moredict:</span><br><span class="line">                                negcount *= <span class="number">2</span></span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.ishdict:</span><br><span class="line">                                negcount *= <span class="number">0.5</span></span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.insufficientlydict:</span><br><span class="line">                                negcount *= -<span class="number">0.3</span></span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.overdict:</span><br><span class="line">                                negcount *= -<span class="number">0.5</span></span><br><span class="line">                            <span class="keyword">elif</span> w <span class="keyword">in</span> hp.inversedict:</span><br><span class="line">                                d += <span class="number">1</span></span><br><span class="line">                            <span class="keyword">else</span>:</span><br><span class="line">                                negcount *= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> tool.is_odd(d) == <span class="string">'odd'</span>:</span><br><span class="line">                        negcount *= -<span class="number">1.0</span></span><br><span class="line">                        negcount2 += negcount</span><br><span class="line">                        negcount = <span class="number">0</span></span><br><span class="line">                        negcount3 = negcount + negcount2 + negcount3</span><br><span class="line">                        negcount2 = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        negcount3 = negcount + negcount2 + negcount3</span><br><span class="line">                        negcount = <span class="number">0</span></span><br><span class="line">                    a = i + <span class="number">1</span>      </span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">                pos_count = poscount3</span><br><span class="line">                neg_count = negcount3</span><br><span class="line">                count1.append([pos_count,neg_count])</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> words[-<span class="number">1</span>] <span class="keyword">in</span> [<span class="string">'!'</span>,<span class="string">'！'</span>]:<span class="comment"># 扫描感叹号前的情感词，发现后权值*2</span></span><br><span class="line">                count1 = [[j*<span class="number">2</span> <span class="keyword">for</span> j <span class="keyword">in</span> c] <span class="keyword">for</span> c <span class="keyword">in</span> count1]</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">for</span> w_im <span class="keyword">in</span> [<span class="string">'但是'</span>,<span class="string">'但'</span>]:</span><br><span class="line">                <span class="keyword">if</span> w_im <span class="keyword">in</span> words : <span class="comment"># 扫描但是后面的情感词，发现后权值*5</span></span><br><span class="line">                    ind = words.index(w_im)</span><br><span class="line">                    count1_head = count1[:ind]</span><br><span class="line">                    count1_tail = count1[ind:]            </span><br><span class="line">                    count1_tail_new = [[j*<span class="number">5</span> <span class="keyword">for</span> j <span class="keyword">in</span> c] <span class="keyword">for</span> c <span class="keyword">in</span> count1_tail]</span><br><span class="line">                    count1 = []</span><br><span class="line">                    count1.extend(count1_head)</span><br><span class="line">                    count1.extend(count1_tail_new)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> words[-<span class="number">1</span>] <span class="keyword">in</span> [<span class="string">'?'</span>,<span class="string">'？'</span>,]:<span class="comment"># 扫描是否有问好，发现后为负面</span></span><br><span class="line">                count1 = [[<span class="number">0</span>,<span class="number">2</span>]]</span><br><span class="line">    </span><br><span class="line">            count2.append(count1)</span><br><span class="line">            count1=[]</span><br><span class="line">        <span class="keyword">return</span> count2</span><br><span class="line">      </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sentiment_score</span>(<span class="params">self,s</span>):</span><br><span class="line">        senti_score_list = self.sentiment_score_list(s)</span><br><span class="line">        <span class="keyword">if</span> senti_score_list != []:</span><br><span class="line">            negatives=[]</span><br><span class="line">            positives=[]</span><br><span class="line">            <span class="keyword">for</span> review <span class="keyword">in</span> senti_score_list:</span><br><span class="line">                score_array =  np.array(review)</span><br><span class="line">                AvgPos = np.<span class="built_in">sum</span>(score_array[:,<span class="number">0</span>])</span><br><span class="line">                AvgNeg = np.<span class="built_in">sum</span>(score_array[:,<span class="number">1</span>])        </span><br><span class="line">                negatives.append(AvgNeg)</span><br><span class="line">                positives.append(AvgPos)   </span><br><span class="line">            pos_score = np.mean(positives) </span><br><span class="line">            neg_score = np.mean(negatives)</span><br><span class="line">            <span class="keyword">if</span> pos_score &gt;=<span class="number">0</span> <span class="keyword">and</span>  neg_score&lt;=<span class="number">0</span>:</span><br><span class="line">                pos_score = pos_score</span><br><span class="line">                neg_score = <span class="built_in">abs</span>(neg_score)</span><br><span class="line">            <span class="keyword">elif</span> pos_score &gt;=<span class="number">0</span> <span class="keyword">and</span>  neg_score&gt;=<span class="number">0</span>:</span><br><span class="line">                pos_score = pos_score</span><br><span class="line">                neg_score = neg_score    </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pos_score,neg_score=<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> pos_score,neg_score</span><br><span class="line">       </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normalization_score</span>(<span class="params">self,sent</span>):</span><br><span class="line">        score1,score0 = self.sentiment_score(sent)</span><br><span class="line">        <span class="keyword">if</span> score1 &gt; <span class="number">4</span> <span class="keyword">and</span> score0 &gt; <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">if</span> score1 &gt;= score0:</span><br><span class="line">                _score1 = <span class="number">1</span></span><br><span class="line">                _score0 = score0/score1    </span><br><span class="line">            <span class="keyword">elif</span> score1 &lt; score0:</span><br><span class="line">                _score0 = <span class="number">1</span></span><br><span class="line">                _score1 = score1/score0  </span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="keyword">if</span> score1 &gt;= <span class="number">4</span> :</span><br><span class="line">                _score1 = <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> score1 &lt; <span class="number">4</span> :</span><br><span class="line">                _score1 = score1/<span class="number">4</span></span><br><span class="line">            <span class="keyword">if</span> score0 &gt;= <span class="number">4</span> :</span><br><span class="line">                _score0 = <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> score0 &lt; <span class="number">4</span> :</span><br><span class="line">                _score0 = score0/<span class="number">4</span> </span><br><span class="line">        <span class="keyword">return</span> _score1,_score0</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">'__main__'</span>:</span><br><span class="line">    sa = SentimentAnalysis()</span><br><span class="line">    text = <span class="string">'根本买不到'</span></span><br><span class="line">    <span class="built_in">print</span>(sa.normalization_score(text))</span><br></pre></td></tr></tbody></table></figure>

<ol start="4">
<li>predict.py文件</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Tue Jan  7 10:28:41 2020</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: cm</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> networks <span class="keyword">import</span> SentimentAnalysis</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">non_bmp_map = <span class="built_in">dict</span>.fromkeys(<span class="built_in">range</span>(<span class="number">0x10000</span>, sys.maxunicode + <span class="number">1</span>), <span class="number">0xfffd</span>)</span><br><span class="line"></span><br><span class="line">SA = SentimentAnalysis()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">sent</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    1: positif</span></span><br><span class="line"><span class="string">    0: neutral</span></span><br><span class="line"><span class="string">    -1: negatif</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    score1,score0 = SA.normalization_score(sent)</span><br><span class="line">    <span class="keyword">if</span> score1 == score0:</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">    <span class="keyword">elif</span> score1 &gt; score0:</span><br><span class="line">        result = <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> score1 &lt; score0:</span><br><span class="line">        result = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> score1-score0</span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean</span>(<span class="params">comment</span>):</span><br><span class="line">    comment = <span class="built_in">str</span>(comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"转发微博"</span>,<span class="string">""</span>, comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"//.*?:"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">'@.*?&lt;/a&gt;'</span>,<span class="string">''</span>,new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"@.*? "</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"@.*?:"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"[.*?]"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"@[\\u4e00-\\u9fa5\\w\\-]+"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">'回复.*?&gt;:'</span>,<span class="string">''</span>,new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">'&lt;.*?&gt;'</span>,<span class="string">''</span>,new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"查看图片"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"【.*?】"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = re.sub(<span class="string">"#.*?#"</span>,<span class="string">""</span>, new_comment)</span><br><span class="line">    new_comment = new_comment.translate(non_bmp_map)</span><br><span class="line">    <span class="keyword">return</span> new_comment</span><br></pre></td></tr></tbody></table></figure>

<p>运行以下程序调用上述文件进行批量处理：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sentiment_classify</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    获取文本的感情偏向（消极 or 积极 or 中立）</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    text:str 本文</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    raw = {<span class="string">"text"</span>:<span class="string">"内容"</span>}</span><br><span class="line">    raw[<span class="string">'text'</span>] = text</span><br><span class="line">    data = json.dumps(raw).encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    AT = <span class="string">""</span> <span class="comment">#input your access token</span></span><br><span class="line">    host = <span class="string">"https://aip.baidubce.com/rpc/2.0/nlp/v1/sentiment_classify?charset=UTF-8&amp;access_token="</span>+AT</span><br><span class="line">    request = urllib.request.Request(url=host,data=data)</span><br><span class="line">    request.add_header(<span class="string">'Content-Type'</span>, <span class="string">'application/json'</span>)</span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    rdata = json.loads(content)</span><br><span class="line">    <span class="keyword">return</span> rdata</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    file = <span class="string">'file_path'</span> <span class="comment">#输入自己的文件路径</span></span><br><span class="line">    text = pd.read_excel(file)</span><br><span class="line">    review = text[<span class="string">'comment_content'</span>]</span><br><span class="line">    length = <span class="built_in">len</span>(review)</span><br><span class="line">    <span class="comment">#初始化用来存储情感分析结果的列表</span></span><br><span class="line">    sentiment = [<span class="string">'blank'</span>]*length</span><br><span class="line">    confidence = [<span class="string">'blank'</span>]*length</span><br><span class="line">    positive_prob = [<span class="string">'blank'</span>]*length</span><br><span class="line"></span><br><span class="line">    time_start = time.time()<span class="comment">#计时</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> review:</span><br><span class="line">        <span class="keyword">if</span> content == <span class="string">''</span>:</span><br><span class="line">           <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            content = content[:<span class="number">512</span>]<span class="comment">#百度api限制512个字符，过长也会导致出错</span></span><br><span class="line">            op = <span class="literal">True</span> <span class="comment">#利用循环和输出条件来保证获取到情绪分析的结果</span></span><br><span class="line">            <span class="keyword">while</span> op:</span><br><span class="line">                maxTryNum = <span class="number">50</span> <span class="comment">#设置最大尝试访问的次数，通过多次访问保证不会因为访问受限制而得不到结果（可修改）</span></span><br><span class="line">                <span class="keyword">for</span> tries <span class="keyword">in</span> <span class="built_in">range</span>(maxTryNum):</span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        result = sentiment_classify(content)</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    <span class="keyword">except</span>:</span><br><span class="line">                        <span class="keyword">if</span> tries &lt; (maxTryNum - <span class="number">1</span>):</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            <span class="built_in">print</span>(<span class="string">'尝试了 %d 次都失败了！！！'</span>,maxTryNum)</span><br><span class="line">                            <span class="keyword">break</span></span><br><span class="line">                <span class="comment">#因为发现如果能够成功调用api则输出结果长度为3，失败了长度为2，故将其设为控制输出的条件</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(result)==<span class="number">3</span>:</span><br><span class="line">                    op = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    op = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">            result1 = result.get(<span class="string">'items'</span>)</span><br><span class="line">            item = result1[<span class="number">0</span>]</span><br><span class="line">            sentiment[i] = item[<span class="string">'sentiment'</span>]</span><br><span class="line">            confidence[i] = item[<span class="string">'confidence'</span>]</span><br><span class="line">            positive_prob[i] = item[<span class="string">'positive_prob'</span>]</span><br><span class="line"></span><br><span class="line">                    <span class="comment">#方便观察进度</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">'第 '</span> + <span class="built_in">str</span>(i+<span class="number">1</span>) + <span class="string">' 条评论已分析完成， 一共 '</span> + <span class="built_in">str</span>(length) + <span class="string">' 条评论'</span>)</span><br><span class="line">            i = i+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    time_end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'分析评论一共耗时：'</span> ,time_end-time_start)</span><br><span class="line"></span><br><span class="line">    text[<span class="string">'sentiment'</span>] = sentiment</span><br><span class="line">    text[<span class="string">'confidence'</span>] = confidence</span><br><span class="line">    text[<span class="string">'positive_prob'</span>] = positive_prob</span><br><span class="line"></span><br><span class="line">    <span class="comment">#保存</span></span><br><span class="line">    text.to_excel(<span class="string">'save_path'</span>,index=<span class="literal">None</span>)</span><br><span class="line">    <span class="built_in">print</span>(file + <span class="string">"    result写入成功!"</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>最终所有文件获取情感倾向成功！进行人工核对后获得最终的训练数据共11083条。</p>
<p><img src="/pic/graduation_issue/image-20230807221441612.png" alt="image-20230807221441612"></p>
<h3 id="5-文本特征提取—Word2vec"><a href="#5-文本特征提取—Word2vec" class="headerlink" title="5. 文本特征提取—Word2vec"></a>5. 文本特征提取—Word2vec</h3><p>参考文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44186838/article/details/117995029">轻松搞懂Word2vec / FastText+BiLSTM、TextCNN、CNN+BiLSTM、BiLSTM+Attention实现中英文情感分类_报告，今天也有好好学习的博客-CSDN博客</a></p>
<h4 id="5-1-简述理论"><a href="#5-1-简述理论" class="headerlink" title="5.1 简述理论"></a>5.1 简述理论</h4><p>文本特征提取说白了就是将自然语言转化为计算机能识别并计算的数据，这里我们用到的方法是Word2vec模型。Word2vec就是采用Distributed Representation构建词向量的工具。它包含CBOW(连续词袋模型)和Skip-gram(跳字模型)，前者利用上下文预测当前位置的词语，后者利用当前位置的词语来预测上下文。它的本质还是神经网络训练，以Skip-gram为例：输入当前词语的One-hot编码，经过网络结构将输出与上下文的One-hot编码得到误差，将误差返回进行迭代更新。</p>
<p>假设语料库中有V个单词，最终想获得的词向量维度为N。滑动窗口为A，则选取的上下文词个数C=2A. 训练过程可以简述为：</p>
<ol>
<li><p>在所有单词中扫描得到上下文单词的one-hot编码；</p>
</li>
<li><p>初始化输入层与隐藏层的矩阵W，并与输入矩阵相乘，得到C个1*N的向量后求平均，得到隐藏层向量h；</p>
</li>
<li><p>初始化隐藏层与输出层的矩阵W’，并与隐藏层相乘，得到的结果经过softmax层转化为概率向量；</p>
</li>
<li><p>向量中概率最大的位置代表的单词即为模型预测输出，将它与实际输出对比得到误差；</p>
</li>
<li><p>定义损失函数，通过最小化损失函数更新W和W’，网络收敛时训练完成。</p>
</li>
<li><p>此时矩阵W就是想要的词向量矩阵。</p>
</li>
</ol>
<p><img src="/pic/graduation_issue/image-20230808201732526.png" alt="image-20230808201732526"></p>
<p><img src="/pic/graduation_issue/image-20230808201739892.png" alt="image-20230808201739892"></p>
<h4 id="5-2-获取词向量"><a href="#5-2-获取词向量" class="headerlink" title="5.2 获取词向量"></a>5.2 获取词向量</h4><p>Python可以直接调用gensim模块函数用所有评论文本训练word2vec模型进行文本特征提取得到每个词语对应的词向量。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建数据通道，进行数据加载，在此之前数据已经切分完毕，一行一篇切好词的文章(词与词之间用空格间隔开)</span></span><br><span class="line">sentences = LineSentence(<span class="string">'weibosentiment.txt'</span>)</span><br><span class="line">logging.basicConfig(<span class="built_in">format</span>=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br><span class="line">model = Word2Vec(sentences, sg=<span class="number">1</span>,vector_size=<span class="number">200</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, workers=<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型以便调用（有多种方法保存模型）</span></span><br><span class="line">model.save(<span class="string">'Dweibocomment.model'</span>)</span><br><span class="line">model.wv.save_word2vec_format(<span class="string">'weibocomment.vector'</span>)</span><br><span class="line"></span><br><span class="line">data = LineSentence(<span class="string">'weibosentiment.txt'</span>)</span><br><span class="line">model  = Word2Vec.load(<span class="string">'weibocomment.model'</span>)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="5-3-句子特征获取"><a href="#5-3-句子特征获取" class="headerlink" title="5.3 句子特征获取"></a>5.3 句子特征获取</h4><p>为了使得句子特征更满足机器学习算法输入的格式，我们还需要获得句子的特征向量。将词向量转化为语句的向量的方法之一是先筛选句子中的所有词语对应的词向量，并将所有词向量取平均值作为该语句对应的特征向量。另一种方法通过<strong>构建索引词典和向量词典</strong>，将句子特征以序号向量的方式输入。这里采用后者获取句子特征，下面简述其方法：</p>
<p>假设词${w}<em>{i}$对应的词向量为${v}</em>{i}$，其中${i}$表示该词语在上述word2vec模型特征词中的索引，则由$\left{w_i:i \right}$构成索引词典， $\left{w_i:v_i \right}$ 构成词向量词典。此时若句子中包含的单词按顺序分别为：$\left[ w_{i_1},w_{i_2},…,w_{i_n} \right]$，则由索引词典知对应的句子特征即为$\left[{i_1},{i_2},…,{i_n}\right]$  , 从而每句评论都得到了其对应的句特征作为输入，并通过词向量字典建立嵌入层，嵌入层将索引还原为对应的词向量矩阵。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_dictionaries</span>(<span class="params">model=<span class="literal">None</span>, data=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">"""将数据集文本转换成句向量，并得到两个词典（单词to序号、单词to向量）</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    model : 训练好的word2vec模型</span></span><br><span class="line"><span class="string">    data : 分词后的文本列表</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    w2indx</span></span><br><span class="line"><span class="string">        数据集中所有单词映射成序号的词典</span></span><br><span class="line"><span class="string">    w2vec</span></span><br><span class="line"><span class="string">        数据集中所有单词映射成向量的词典</span></span><br><span class="line"><span class="string">    data</span></span><br><span class="line"><span class="string">        数据集文本特征矩阵</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> (data <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (model <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">        gensim_dict = Dictionary()</span><br><span class="line">        gensim_dict.doc2bow(model.wv.index_to_key,</span><br><span class="line">                            allow_update=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#  freqxiao10-&gt;0 所以k+1</span></span><br><span class="line">        w2indx = {v: k + <span class="number">1</span> <span class="keyword">for</span> k, v <span class="keyword">in</span> gensim_dict.items()}  <span class="comment"># 频数超过10的词语的索引</span></span><br><span class="line">        f = <span class="built_in">open</span>(<span class="string">"word2index.txt"</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf8'</span>)</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> w2indx:</span><br><span class="line">            f.write(<span class="built_in">str</span>(key))</span><br><span class="line">            f.write(<span class="string">' '</span>)</span><br><span class="line">            f.write(<span class="built_in">str</span>(w2indx[key]))</span><br><span class="line">            f.write(<span class="string">'\n'</span>)</span><br><span class="line">        f.close()</span><br><span class="line">        w2vec = {word: model.wv[word] <span class="keyword">for</span> word <span class="keyword">in</span> w2indx.keys()}  <span class="comment"># 频数超过10的词语的词向量</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">parse_dataset</span>(<span class="params">combined</span>):</span><br><span class="line">            data = []</span><br><span class="line">            <span class="keyword">for</span> sentence <span class="keyword">in</span> combined:</span><br><span class="line">                new_txt = []</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        new_txt.append(w2indx[word])</span><br><span class="line">                    <span class="keyword">except</span>:</span><br><span class="line">                        new_txt.append(<span class="number">0</span>)</span><br><span class="line">                data.append(new_txt)</span><br><span class="line">            <span class="keyword">return</span> data  <span class="comment"># word=&gt;index</span></span><br><span class="line">        </span><br><span class="line">        maxlen = <span class="number">96</span></span><br><span class="line">        data = parse_dataset(data)</span><br><span class="line">        data = sequence.pad_sequences(data, maxlen=maxlen)  </span><br><span class="line">        <span class="keyword">return</span> w2indx, w2vec, data</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'文本为空！'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">w2indx, w2vec, data, y</span>):</span><br><span class="line">    vocab_dim = <span class="number">200</span></span><br><span class="line">    n_symbols = <span class="built_in">len</span>(w2indx) + <span class="number">1</span>  <span class="comment"># 补上索引为0（频数小于10）的词</span></span><br><span class="line">    embedding_weights = np.zeros((n_symbols, vocab_dim))  </span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> w2indx.items(): </span><br><span class="line">        embedding_weights[index, :] = w2vec[word]</span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(data, y, test_size=<span class="number">0.2</span>)</span><br><span class="line">    <span class="comment">#split = StratifiedShuffleSplit(n_splits=1, test_size=0.2)</span></span><br><span class="line">    <span class="comment">#y = np.array(y)</span></span><br><span class="line">    <span class="comment">#for train_index, test_index in split.split(train_data,y):</span></span><br><span class="line">        <span class="comment">#train_index = train_index</span></span><br><span class="line">        <span class="comment">#test_index = test_index</span></span><br><span class="line">        <span class="comment">#X_train, X_test = train_data[train_index], train_data[test_index]#训练集对应的值</span></span><br><span class="line">        <span class="comment">#y_train, y_test = y[train_index], y[test_index]#类别集对应的值</span></span><br><span class="line">        </span><br><span class="line">    y_train = np_utils.to_categorical(y_train, num_classes=<span class="number">3</span>)  <span class="comment"># 转换为one-hot特征</span></span><br><span class="line">    y_test = np_utils.to_categorical(y_test, num_classes=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> n_symbols, embedding_weights, x_train, y_train, x_test, y_test        </span><br><span class="line">            </span><br><span class="line"><span class="comment"># 数据准备</span></span><br><span class="line">df_train = pd.read_excel(<span class="string">'marked.xlsx'</span>)</span><br><span class="line">y = df_train[<span class="string">'sentiment'</span>].values.tolist()</span><br><span class="line"></span><br><span class="line"><span class="comment">#准备词典</span></span><br><span class="line">w2indx, w2vec, data = create_dictionaries(model, data)</span><br><span class="line">train_data = data[:<span class="number">11083</span>]</span><br><span class="line"></span><br><span class="line">n_symbols, embedding_weights, x_train, y_train, x_test, y_test = get_data(w2indx, w2vec,train_data, y)</span><br></pre></td></tr></tbody></table></figure>



<h3 id="6-情感分类—CNN-BiLSTM"><a href="#6-情感分类—CNN-BiLSTM" class="headerlink" title="6. 情感分类—CNN-BiLSTM"></a>6. 情感分类—CNN-BiLSTM</h3><p>这里选择搭建CNN-BiLSTM神经网络，当然还可以尝试其他神经网络，比如去掉CNN结构，只采用Bi-LSTM模型</p>
<p>CNN-BiLSTM模型由输入层、嵌入层、卷积层、BiLSTM层、全连接层和输出层构成，其中输入层输入句子对应的索引向量，嵌入层将索引向量转化为词向量，卷积层通过卷积核实现对数据特征的提取，BiLSTM层的作用是负责理解句子的语义信息，可以捕捉双向语义依赖性，从而提高模型的准确性和泛化能力。并且在其后加上Dropout层随机丢弃节点，以防过拟合。</p>
<p><img src="/pic/graduation_issue/image-20230808205640018.png" alt="image-20230808205640018"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数设置</span></span><br><span class="line">batch_size = <span class="number">200</span></span><br><span class="line">maxlen = <span class="number">96</span></span><br><span class="line">vocab_dim = <span class="number">200</span></span><br><span class="line">n_epoch = <span class="number">14</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main_input = Input(shape=(maxlen,), dtype=<span class="string">'float64'</span>)</span><br><span class="line">    embedder = Embedding(output_dim=vocab_dim,</span><br><span class="line">                         input_dim=n_symbols,</span><br><span class="line">                         input_length=maxlen,</span><br><span class="line">                         weights=[embedding_weights])</span><br><span class="line">    embed = embedder(main_input)</span><br><span class="line">    cnn = Conv1D(<span class="number">64</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>, strides=<span class="number">1</span>, activation=<span class="string">'relu'</span>)(embed)</span><br><span class="line">    bilstm = Bidirectional(LSTM(units=<span class="number">50</span>, dropout=<span class="number">0.5</span>, activation=<span class="string">'tanh'</span>, return_sequences=<span class="literal">True</span>))(cnn)</span><br><span class="line">    flat = Flatten()(bilstm)</span><br><span class="line">    main_output = Dense(<span class="number">3</span>, activation=<span class="string">'softmax'</span>)(flat)</span><br><span class="line">    model = Model(inputs=main_input, outputs=main_output)</span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch)</span><br><span class="line">    model.save(<span class="string">'cnnbilstm_final.h5'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 训练过程可视化</span></span><br><span class="line">hist = history.history</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">ax.plot(hist[<span class="string">'loss'</span>], lw=<span class="number">3</span>)</span><br><span class="line">ax.set_title(<span class="string">'Training loss'</span>, size=<span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Epoch'</span>, size=<span class="number">15</span>)</span><br><span class="line">ax.grid()</span><br><span class="line">ax.tick_params(axis=<span class="string">'both'</span>, which=<span class="string">'major'</span>, labelsize=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">ax.plot(hist[<span class="string">'accuracy'</span>], lw=<span class="number">3</span>)</span><br><span class="line">ax.set_title(<span class="string">'Training accuracy'</span>, size=<span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Epoch'</span>, size=<span class="number">15</span>)</span><br><span class="line">ax.tick_params(axis=<span class="string">'both'</span>, which=<span class="string">'major'</span>, labelsize=<span class="number">15</span>)</span><br><span class="line">ax.grid()</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">'cnnbilstm-learning-curve.png'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>训练集上结果：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/14</span><br><span class="line">45/45 [==============================] - 23s 413ms/step - loss: 0.8576 - accuracy: 0.5970</span><br><span class="line">Epoch 2/14</span><br><span class="line">45/45 [==============================] - 24s 532ms/step - loss: 0.6369 - accuracy: 0.7391</span><br><span class="line">Epoch 3/14</span><br><span class="line">45/45 [==============================] - 24s 521ms/step - loss: 0.4933 - accuracy: 0.8037</span><br><span class="line">Epoch 4/14</span><br><span class="line">45/45 [==============================] - 23s 512ms/step - loss: 0.3971 - accuracy: 0.8437</span><br><span class="line">Epoch 5/14</span><br><span class="line">45/45 [==============================] - 23s 518ms/step - loss: 0.3358 - accuracy: 0.8704</span><br><span class="line">Epoch 6/14</span><br><span class="line">45/45 [==============================] - 23s 514ms/step - loss: 0.2870 - accuracy: 0.8883</span><br><span class="line">Epoch 7/14</span><br><span class="line">45/45 [==============================] - 24s 523ms/step - loss: 0.2540 - accuracy: 0.9038</span><br><span class="line">Epoch 8/14</span><br><span class="line">45/45 [==============================] - 24s 526ms/step - loss: 0.2258 - accuracy: 0.9118</span><br><span class="line">Epoch 9/14</span><br><span class="line">45/45 [==============================] - 25s 547ms/step - loss: 0.2003 - accuracy: 0.9229</span><br><span class="line">Epoch 10/14</span><br><span class="line">45/45 [==============================] - 24s 527ms/step - loss: 0.1742 - accuracy: 0.9320</span><br><span class="line">Epoch 11/14</span><br><span class="line">45/45 [==============================] - 24s 529ms/step - loss: 0.1523 - accuracy: 0.9419</span><br><span class="line">Epoch 12/14</span><br><span class="line">45/45 [==============================] - 24s 536ms/step - loss: 0.1394 - accuracy: 0.9483</span><br><span class="line">Epoch 13/14</span><br><span class="line">45/45 [==============================] - 25s 549ms/step - loss: 0.1225 - accuracy: 0.9545</span><br><span class="line">Epoch 14/14</span><br><span class="line">45/45 [==============================] - 24s 539ms/step - loss: 0.1217 - accuracy: 0.9536</span><br></pre></td></tr></tbody></table></figure>

<p><img src="/pic/graduation_issue/%E4%B8%8B%E8%BD%BD.png" alt="下载"></p>
<p>在测试集上准确率在80%左右，表现可以接受，但与训练集上表现有一定差距，可以观察测试集上loss值更改参数或者采用新模型，这里将重点放在后续情感分析的过程上，故不再优化。</p>
<h3 id="7-结果分析"><a href="#7-结果分析" class="headerlink" title="7. 结果分析"></a>7. 结果分析</h3><h4 id="7-1-总体分析"><a href="#7-1-总体分析" class="headerlink" title="7.1 总体分析"></a>7.1 总体分析</h4><p>总体来看，评论中大多数评论表现出负向情绪，其占比为56%，其次是中性情绪约占30%，而正面情绪最少，仅占14%. 这说明“新十条”疫情防控政策发布后，公众消极情绪较为强烈。</p>
<p><img src="/pic/graduation_issue/image-20230808212516269.png" alt="image-20230808212516269"></p>
<p>探索影响用户情感倾向的因素，发现不同性别、不同阳光信用等级、不同ip地址的用户情感均值差异不明显。将用户根据粉丝数划分为10个等级，每个等级中包含的用户数、每个等级的用户情感平均值如图16，结果显示，粉丝数与情感倾向有一定的关系，粉丝数多的用户群体的整体情感倾向也更加积极。</p>
<p><img src="/pic/graduation_issue/image-20230808212549077.png" alt="image-20230808212549077"></p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">负面情绪</th>
<th align="center">中性情绪</th>
<th align="center">正面情绪</th>
</tr>
</thead>
<tbody><tr>
<td align="center">平均粉丝数</td>
<td align="center">3262.641</td>
<td align="center">10333.11</td>
<td align="center">11913.75</td>
</tr>
<tr>
<td align="center">粉丝中位数</td>
<td align="center">83</td>
<td align="center">99</td>
<td align="center">111</td>
</tr>
</tbody></table>
<h4 id="7-2-词频统计"><a href="#7-2-词频统计" class="headerlink" title="7.2 词频统计"></a>7.2 词频统计</h4><p>为了进一步分析不同情感倾向评论的重点，绘制不同情感倾向的评论词云如图</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]<span class="comment">#用来显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span><span class="comment">#用来正常显示负号</span></span><br><span class="line"></span><br><span class="line">df = pd.read_excel(file_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 引入停用词</span></span><br><span class="line">stop_words = []</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'D:\BaiduNetdiskDownload\comment\stopwords.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fr:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> fr:</span><br><span class="line">        stop_words.append(i.strip(<span class="string">'\n'</span>))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">drop_stopwords</span>(<span class="params">contents,stopwords</span>):</span><br><span class="line">    contents_clean = []</span><br><span class="line">    all_words = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> contents:</span><br><span class="line">        line_clean = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> line:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> stopwords:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            line_clean.append(word)</span><br><span class="line">            all_words.append(<span class="built_in">str</span>(word))</span><br><span class="line">        contents_clean.append(line_clean)</span><br><span class="line">    <span class="keyword">return</span> contents_clean,all_words</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pre_sep</span>(<span class="params">df</span>):</span><br><span class="line">    <span class="comment"># 分词</span></span><br><span class="line">    txt = df.comment_content.values.tolist()</span><br><span class="line">    content_S = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> txt:</span><br><span class="line">        current_segment = jieba.lcut(<span class="built_in">str</span>(line))</span><br><span class="line">        content_S.append(current_segment)</span><br><span class="line">    contents_clean,all_words = drop_stopwords(content_S,stop_words)</span><br><span class="line">    <span class="comment"># contents_clean</span></span><br><span class="line">    <span class="keyword">return</span> (contents_clean)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visual</span>(<span class="params">contents_clean,out_path=<span class="literal">None</span>,mask_in_path=<span class="literal">None</span>,mask_out_path = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># 统计词频</span></span><br><span class="line">    c = Counter()</span><br><span class="line">    <span class="keyword">for</span> txt <span class="keyword">in</span> contents_clean:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> txt:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(j) &gt; <span class="number">1</span> <span class="keyword">and</span> j != <span class="string">'\r\n'</span>:</span><br><span class="line">                c[j] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> out_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"==============================================================="</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Top 20 高频词为:"</span>)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(out_path, <span class="string">'w'</span>, encoding=<span class="string">'gbk'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> (k, v) <span class="keyword">in</span> c.most_common():</span><br><span class="line">                fw.write(k + <span class="string">' '</span> + <span class="built_in">str</span>(v) + <span class="string">'\n'</span>)</span><br><span class="line">                <span class="keyword">if</span> j &lt; <span class="number">20</span>:</span><br><span class="line">                    <span class="built_in">print</span>(k + <span class="string">' '</span> + <span class="built_in">str</span>(v))</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> mask_in_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mask = np.array(Image.<span class="built_in">open</span>(mask_in_path))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        mask = <span class="literal">None</span></span><br><span class="line">    wc = WordCloud(font_path=<span class="string">'simhei.ttf'</span>, background_color=<span class="string">'white'</span>, max_words=<span class="number">2000</span>, mask=mask).fit_words(c)</span><br><span class="line">    plt.imshow(wc)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    <span class="keyword">if</span> mask_out_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        wc.to_file(mask_out_path)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 正负面评论词云图</span></span><br><span class="line">df_postive = df[df[<span class="string">'sentiment'</span>]==<span class="number">2</span>]</span><br><span class="line">df_negative = df[df[<span class="string">'sentiment'</span>]==<span class="number">0</span>]</span><br><span class="line">df_else = df[df[<span class="string">'sentiment'</span>]==<span class="number">1</span>]</span><br><span class="line">cc1 = pre_sep(df_postive)</span><br><span class="line">cc2 = pre_sep(df_negative)</span><br><span class="line">cc3 = pre_sep(df_else)</span><br><span class="line">mask_path=<span class="string">'{}.jpg'</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">18</span>,<span class="number">15</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">131</span>)</span><br><span class="line">ax1.set_title(<span class="string">"正面情绪"</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">visual(cc1,out_path=<span class="literal">None</span>,mask_in_path=mask_path.<span class="built_in">format</span>(<span class="string">'happy'</span>),mask_out_path = mask_path.<span class="built_in">format</span>(<span class="string">'happy_res'</span>))</span><br><span class="line">ax2 = plt.subplot(<span class="number">132</span>)</span><br><span class="line">ax2.set_title(<span class="string">"负面情绪"</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">visual(cc2,out_path=<span class="literal">None</span>,mask_in_path=mask_path.<span class="built_in">format</span>(<span class="string">'unhappy'</span>),mask_out_path = mask_path.<span class="built_in">format</span>(<span class="string">'unhappy_res'</span>))</span><br><span class="line">ax3 = plt.subplot(<span class="number">133</span>)</span><br><span class="line">ax3.set_title(<span class="string">"中性情绪"</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">visual(cc3,out_path=<span class="literal">None</span>,mask_in_path=mask_path.<span class="built_in">format</span>(<span class="string">'neutral'</span>),mask_out_path = mask_path.<span class="built_in">format</span>(<span class="string">'neutral_res'</span>))</span><br><span class="line">plt.savefig(<span class="string">"wordcount_sentiment.jpg"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<p><img src="/pic/graduation_issue/image-20230808213333622.png" alt="image-20230808213333622"></p>
<h4 id="7-3-情感变化"><a href="#7-3-情感变化" class="headerlink" title="7.3 情感变化"></a>7.3 情感变化</h4><p>绘制日均情感值随时间变化的柱状图</p>
<p><img src="/pic/graduation_issue/image-20230808213603155.png" alt="image-20230808213603155"></p>
<p>将时间划为四个阶段：5日-10日、11日-15日、16日-28日、29日-31日，分别绘制这四个阶段内的词频。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不同时期</span></span><br><span class="line">df_first = df[(df[<span class="string">'comment_date'</span>] &gt;= <span class="string">'2022-12-05'</span>)&amp;(df[<span class="string">'comment_date'</span>] &lt; <span class="string">'2022-12-11'</span>)]</span><br><span class="line">df_second = df[(df[<span class="string">'comment_date'</span>] &gt;= <span class="string">'2022-12-11'</span>)&amp;(df[<span class="string">'comment_date'</span>] &lt; <span class="string">'2022-12-16'</span>)]</span><br><span class="line">df_third = df[(df[<span class="string">'comment_date'</span>] &gt;= <span class="string">'2022-12-16'</span>)&amp;(df[<span class="string">'comment_date'</span>] &lt; <span class="string">'2022-12-29'</span>)]</span><br><span class="line">df_fourth = df[(df[<span class="string">'comment_date'</span>] &gt;= <span class="string">'2022-12-29'</span>)&amp;(df[<span class="string">'comment_date'</span>] &lt; <span class="string">'2023-01-01'</span>)]</span><br><span class="line">cc6 = pre_sep(df_first)</span><br><span class="line">cc7 = pre_sep(df_second)</span><br><span class="line">cc8 = pre_sep(df_third)</span><br><span class="line">cc9 = pre_sep(df_fourth)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">221</span>)</span><br><span class="line">visual(cc6,out_path=<span class="literal">None</span>,mask_in_path=<span class="literal">None</span>,mask_out_path = <span class="literal">None</span>)</span><br><span class="line">ax1.set_title(<span class="string">'2022/12/5-2022/12-10'</span>,fontsize=<span class="number">30</span>)</span><br><span class="line">ax2 = plt.subplot(<span class="number">222</span>)</span><br><span class="line">visual(cc7,out_path=<span class="literal">None</span>,mask_in_path=<span class="literal">None</span>,mask_out_path = <span class="literal">None</span>)</span><br><span class="line">ax2.set_title(<span class="string">'2022/12/11-2022/12-15'</span>,fontsize=<span class="number">30</span>)</span><br><span class="line">ax3 = plt.subplot(<span class="number">223</span>)</span><br><span class="line">visual(cc8,out_path=<span class="literal">None</span>,mask_in_path=<span class="literal">None</span>,mask_out_path = <span class="literal">None</span>)</span><br><span class="line">ax3.set_title(<span class="string">'2022/12/16-2022/12-28'</span>,fontsize=<span class="number">30</span>)</span><br><span class="line">ax4 = plt.subplot(<span class="number">224</span>)</span><br><span class="line">visual(cc9,out_path=<span class="literal">None</span>,mask_in_path=<span class="literal">None</span>,mask_out_path = <span class="literal">None</span>)</span><br><span class="line">ax4.set_title(<span class="string">'2022/12/29-2022/12-31'</span>,fontsize=<span class="number">30</span>)</span><br><span class="line">plt.savefig(<span class="string">'time_wordcount.jpg'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<p>结果说明，不同阶段用户关注的重不同：</p>
<p>疫情政策刚刚颁布期间，评论大多围绕过去的防疫和新政策展开；</p>
<p>第二阶段，评论大多是对过去三年抗疫的回忆，包括感慨、感谢、惆怅等情绪，同时“买不到”“专家”两个词出现率很高，这一阶段疫情感染率迅速增长，于是大多数家庭为药品供应不足感到担心；</p>
<p>第三阶段，评论以无症状的定义、自身感染症状的描述为主，同时对“专家”的质疑也越来越多；</p>
<p>第四阶段，此时大多数人已经被感染，评论多为“新冠感染”转化为“新冠肺炎”症状的探讨，部分评论还表达了对老人的担忧。</p>
<p><img src="/pic/graduation_issue/image-20230808213652207.png" alt="image-20230808213652207"></p>
<h4 id="7-4-相似词分析"><a href="#7-4-相似词分析" class="headerlink" title="7.4 相似词分析"></a>7.4 相似词分析</h4><p>利用word2vec得到的词向量，向量间的相似性能够表示词语之间的相似性。因此先从词频统计图中选择此次事件中被谈论最多的主体对象，并利用这一特性，获得主体对象的相似词。</p>
<p>词频图显示被讨论较多的主体对象包含：专家、国家、核酸、疫苗、医院、老人。利用word2vec模型获得上述主体对象的相似词。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> LineSentence</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">wordmodel = Word2Vec.load(<span class="string">'weibocomment.model'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">distance_visual</span>(<span class="params">model,word,xlim,ylim,path=<span class="literal">None</span></span>):</span><br><span class="line">    res = model.wv.most_similar(word,topn=<span class="number">20</span>)</span><br><span class="line">    words,data = <span class="built_in">zip</span>(*res)</span><br><span class="line">    words = <span class="built_in">list</span>(words)</span><br><span class="line">    target = model.wv[word]</span><br><span class="line">    X = []</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> words:</span><br><span class="line">        X.append(model.wv[t])</span><br><span class="line">    X.append(target)</span><br><span class="line">    pca = PCA(n_components = <span class="number">2</span>)</span><br><span class="line">    data = pca.fit_transform(X)</span><br><span class="line">    <span class="comment">#plt.figure( dpi=100)</span></span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.xlim(xlim)</span><br><span class="line">    plt.ylim(ylim)</span><br><span class="line">    plt.scatter(data[:<span class="number">20</span>, <span class="number">0</span>], data[:<span class="number">20</span>, <span class="number">1</span>],marker=<span class="string">'o'</span>)</span><br><span class="line">    plt.scatter(data[<span class="number">20</span>, <span class="number">0</span>], data[<span class="number">20</span>, <span class="number">1</span>],marker=<span class="string">'o'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(words):</span><br><span class="line">        plt.annotate(j, xy=(data[i, <span class="number">0</span>]+<span class="number">0.05</span>, data[i, <span class="number">1</span>]+<span class="number">0.05</span>),fontsize = <span class="number">12</span>)</span><br><span class="line">    plt.annotate(word, xy=(data[i+<span class="number">1</span>, <span class="number">0</span>]+<span class="number">0.05</span>, data[i+<span class="number">1</span>, <span class="number">1</span>]+<span class="number">0.05</span>),fontsize = <span class="number">15</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">10</span>),dpi=<span class="number">100</span>)</span><br><span class="line">ax1 = plt.subplot(<span class="number">231</span>)</span><br><span class="line">distance_visual(wordmodel,<span class="string">'专家'</span>,xlim=(-<span class="number">1.0</span>,<span class="number">1.5</span>),ylim=(-<span class="number">0.5</span>,<span class="number">2</span>))</span><br><span class="line">ax2 = plt.subplot(<span class="number">232</span>)</span><br><span class="line">distance_visual(wordmodel,<span class="string">'国家'</span>,xlim=(-<span class="number">1.0</span>,<span class="number">2.0</span>),ylim=(-<span class="number">0.5</span>,<span class="number">2</span>))</span><br><span class="line">ax3 = plt.subplot(<span class="number">233</span>)</span><br><span class="line">distance_visual(wordmodel,<span class="string">'核酸'</span>,xlim=(-<span class="number">0.5</span>,<span class="number">2.5</span>),ylim=(-<span class="number">0.5</span>,<span class="number">1.5</span>))</span><br><span class="line">ax4 = plt.subplot(<span class="number">234</span>)</span><br><span class="line">distance_visual(wordmodel,<span class="string">'疫苗'</span>,xlim=(<span class="number">0</span>,<span class="number">2.0</span>),ylim=(-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">ax5 = plt.subplot(<span class="number">235</span>)</span><br><span class="line">distance_visual(wordmodel,<span class="string">'医院'</span>,xlim=(-<span class="number">1.0</span>,<span class="number">1.0</span>),ylim=(-<span class="number">0.5</span>,<span class="number">1.5</span>))</span><br><span class="line">ax6 = plt.subplot(<span class="number">236</span>)</span><br><span class="line">distance_visual(wordmodel,<span class="string">'老人'</span>,xlim=(-<span class="number">0.5</span>,<span class="number">2</span>),ylim=(-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">"word2vec_similarity.jpg"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<p>结果显示：</p>
<ul>
<li><p>“专家”的高度相关词包括“砖家”“放屁”“狗屁”等</p>
</li>
<li><p>和“国家“高度相关的词有“政府”“付出”“人民”“祖国”“辛苦”“努力”等</p>
</li>
<li><p>“核酸”的高度相关词包含“检测”“阴性”“两次”“要求”“造假”等</p>
</li>
<li><p>“疫苗”的高度相关词包含“没打”“打过”“奥密克戎”“容易”“科兴”“抗体”等</p>
</li>
<li><p>“医院”的高度相关词汇有“医生”“床位”“方舱”“检查”，</p>
</li>
<li><p>“老人”的相关词有“小孩”“家里”“孩子”“年轻人”“老年人”“担心”“基础病”</p>
</li>
</ul>
<p><img src="/pic/graduation_issue/image-20230808214151767.png" alt="image-20230808214151767"></p>
<p>同时，统计含有这些词语的评论情感倾向分布：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_excel(<span class="string">"final.xlsx"</span>)</span><br><span class="line">comments = df[<span class="string">'comment_content'</span>].values.tolist()</span><br><span class="line">sentiments = df[<span class="string">'sentiment'</span>].values.tolist()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">object_senti</span>(<span class="params">objects</span>):</span><br><span class="line">    senti_list= [sentiments[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(comments)) <span class="keyword">if</span> objects <span class="keyword">in</span> <span class="built_in">str</span>(comments[i])]</span><br><span class="line">    <span class="keyword">return</span> senti_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">iter_count</span>(<span class="params"><span class="built_in">list</span></span>):</span><br><span class="line">    neg = <span class="number">0</span></span><br><span class="line">    pos = <span class="number">0</span></span><br><span class="line">    neu = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>:</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            neg += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">2</span>:</span><br><span class="line">            pos += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            neu += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(<span class="built_in">list</span>),neg,neu,pos</span><br><span class="line"></span><br><span class="line">objects = [<span class="string">"专家"</span>,<span class="string">"国家"</span>,<span class="string">"核酸"</span>,<span class="string">"疫苗"</span>,<span class="string">"医院"</span>,<span class="string">"疫情"</span>,<span class="string">"新冠"</span>,<span class="string">"老人"</span>]</span><br><span class="line"></span><br><span class="line">count_lis = []</span><br><span class="line">neg_lis = []</span><br><span class="line">neu_lis = []</span><br><span class="line">pos_lis = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> objects:</span><br><span class="line">    senti_list = object_senti(k)</span><br><span class="line">    count,neg,neu,pos = iter_count(senti_list)</span><br><span class="line">    count_lis.append(count)</span><br><span class="line">    neg_lis.append(neg)</span><br><span class="line">    neu_lis.append(neu)</span><br><span class="line">    pos_lis.append(pos)</span><br><span class="line">obj_senti= pd.DataFrame({<span class="string">"count"</span>:count_lis,<span class="string">"neg_count"</span>:neg_lis,<span class="string">"neu_count"</span>:neu_lis,<span class="string">"pos_count"</span>:pos_lis})</span><br><span class="line">obj_senti.index = objects</span><br><span class="line">obj_senti</span><br></pre></td></tr></tbody></table></figure>

<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">count</th>
<th align="center">neg_count</th>
<th align="center">neu_count</th>
<th align="center">pos_count</th>
</tr>
</thead>
<tbody><tr>
<td align="center">专家</td>
<td align="center">1237</td>
<td align="center">1152</td>
<td align="center">67</td>
<td align="center">18</td>
</tr>
<tr>
<td align="center">国家</td>
<td align="center">738</td>
<td align="center">249</td>
<td align="center">90</td>
<td align="center">399</td>
</tr>
<tr>
<td align="center">核酸</td>
<td align="center">575</td>
<td align="center">294</td>
<td align="center">264</td>
<td align="center">17</td>
</tr>
<tr>
<td align="center">疫苗</td>
<td align="center">576</td>
<td align="center">404</td>
<td align="center">157</td>
<td align="center">15</td>
</tr>
<tr>
<td align="center">医院</td>
<td align="center">578</td>
<td align="center">439</td>
<td align="center">118</td>
<td align="center">21</td>
</tr>
<tr>
<td align="center">疫情</td>
<td align="center">528</td>
<td align="center">262</td>
<td align="center">178</td>
<td align="center">88</td>
</tr>
<tr>
<td align="center">新冠</td>
<td align="center">596</td>
<td align="center">419</td>
<td align="center">149</td>
<td align="center">28</td>
</tr>
<tr>
<td align="center">老人</td>
<td align="center">349</td>
<td align="center">283</td>
<td align="center">48</td>
<td align="center">18</td>
</tr>
</tbody></table>
<p><img src="/pic/graduation_issue/image-20230808214558092.png" alt="image-20230808214558092"></p>

        </div>

        
            <section class="post-copyright">
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://example.com/2023/08/18/graduation-issue/">http://example.com/2023/08/18/graduation-issue/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Python/"># Python</a>
                    
                        <a href="/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"># 情感分析</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2023/08/23/task2/">AI夏令营-用户新增预测挑战赛教程——任务2</a>
            
            
            <a class="next" rel="next" href="/2023/08/18/task1/">AI夏令营-用户新增预测挑战赛教程——任务1</a>
            
        </section>


    </article>
</div>

  
 <div id="gitalk-container"></div>  
 
<link rel="stylesheet" href="//unpkg.com/gitalk/dist/gitalk.css">
<script src="//unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
 var gitalk = new Gitalk({  
 clientID: 'b647c2f85009b1355e1d',  
 clientSecret: 'b034ae0bf07e04f8819c52eb0e948a378dcbf524',  
 repo: 'Elvais.github.io',  
 owner: 'Elvais',  
 admin: 'Elvais',  
 id: location.pathname,  
 labels: 'Gitalk'.split(',').filter(l => l),  
 perPage: 10,  
 pagerDirection: 'last',  
 proxy: 'https://netnr-proxy.cloudno.de/https://github.com/login/oauth/access_token',
 createIssueManually: true,  
 distractionFreeMode: true  
 })  
 gitalk.render('gitalk-container')  
</script>  

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© WuXin | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>